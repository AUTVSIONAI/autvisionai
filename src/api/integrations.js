// ğŸ”¥ INTEGRAÃ‡ÃƒO LLM REAL COM BACKEND AUTVISION
import axios from "./client";

// ğŸŒ ConfiguraÃ§Ã£o baseada em variÃ¡veis de ambiente
const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'https://autvisionai-backend-five.vercel.app';
const LLM_MOCK_MODE = import.meta.env.VITE_LLM_MOCK_MODE === 'true';
const ENVIRONMENT = import.meta.env.VITE_ENVIRONMENT || 'development';

console.log('ğŸ”§ LLM Config:', { 
  API_BASE_URL, 
  LLM_MOCK_MODE, 
  ENVIRONMENT 
});

/**
 * ğŸ§  Invoca a LLM atravÃ©s do backend AutVision
 * @param {Object} params - ParÃ¢metros da chamada LLM
 * @param {string} params.prompt - Prompt para a LLM
 * @param {string} [params.systemPrompt] - System prompt opcional
 * @param {string} [params.agentId] - ID do agente (opcional)
 * @param {Object} [params.context] - Contexto adicional
 * @param {Object} [params.response_json_schema] - Schema JSON esperado (serÃ¡ usado no system prompt)
 * @returns {Promise<Object>} Resposta da LLM
 */
export const InvokeLLM = async ({ 
  prompt, 
  systemPrompt, 
  context,
  response_json_schema 
}) => {
  // Se estiver em modo mock, retorna resposta mock diretamente
  if (LLM_MOCK_MODE) {
    console.log('ğŸ¤– Modo Mock ativado, retornando resposta simulada');
    return generateMockResponse(prompt, response_json_schema);
  }

  try {
    // Se hÃ¡ schema JSON, ajustar o system prompt para solicitar JSON
    let finalSystemPrompt = systemPrompt;
    if (response_json_schema && !finalSystemPrompt?.includes('JSON')) {
      finalSystemPrompt = `${systemPrompt || 'VocÃª Ã© um assistente Ãºtil.'}\n\nIMPORTANTE: Responda APENAS com um JSON vÃ¡lido seguindo exatamente este schema:\n${JSON.stringify(response_json_schema, null, 2)}`;
    }

    console.log('ğŸ§  Enviando prompt para LLM:', prompt.substring(0, 100) + '...');

    const response = await axios.post('/llm/invoke', {
      prompt,
      options: {
        systemPrompt: finalSystemPrompt,
        temperature: 0.7,
        maxTokens: 2048
      },
      context: {
        source: 'vision-commander',
        timestamp: new Date().toISOString(),
        environment: ENVIRONMENT,
        ...context
      }
    }, { 
      timeout: 15000, // Timeout de 15s para produÃ§Ã£o
      baseURL: API_BASE_URL
    });

    if (response.data?.success) {
      const llmResponse = response.data.data.content || response.data.data.response;
      
      // Se esperamos JSON, tentar fazer parse
      if (response_json_schema) {
        try {
          // Extrair JSON se a resposta contÃ©m texto + JSON
          const jsonMatch = llmResponse.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            return JSON.parse(jsonMatch[0]);
          } else {
            return JSON.parse(llmResponse);
          }
        } catch (parseError) {
          console.warn('âŒ Resposta nÃ£o Ã© JSON vÃ¡lido, retornando como texto:', parseError);
          return { response: llmResponse, error: 'Invalid JSON response' };
        }
      }

      // Retorna resposta como string se nÃ£o Ã© JSON
      return { 
        response: llmResponse,
        modelUsed: response.data.data.modelUsed || 'IA',
        tokensUsed: response.data.data.tokensUsed || 0
      };
    } else {
      throw new Error(response.data?.error || 'Erro na resposta da LLM');
    }

  } catch (error) {
    console.error('âŒ Erro ao chamar LLM:', error);
    
    // Fallback para desenvolvimento quando backend nÃ£o estÃ¡ disponÃ­vel
    if (error.response?.status === 404 || 
        error.code === 'ECONNREFUSED' || 
        error.code === 'ECONNABORTED' || 
        error.message?.includes('Network Error') ||
        error.message?.includes('timeout')) {
      
      console.warn('âš ï¸ Backend nÃ£o disponÃ­vel, usando resposta mock');
      return generateMockResponse(prompt, response_json_schema);
    }
    
    throw error;
  }
};

/**
 * ğŸ­ Gera resposta mock inteligente baseada no prompt e schema
 */
function generateMockResponse(prompt, response_json_schema) {
  // Se esperamos JSON estruturado
  if (response_json_schema) {
    return generateMockJsonResponse(response_json_schema);
  }

  // Respostas mock baseadas no contexto do prompt
  const promptLower = prompt.toLowerCase();
  
  if (promptLower.includes('analise') || promptLower.includes('anÃ¡lise')) {
    return {
      response: 'Sistema AUTVISION funcionando perfeitamente em modo de desenvolvimento. Backend serÃ¡ conectado em breve. Todas as funcionalidades core estÃ£o operacionais.',
      modelUsed: 'MockAI Dev',
      tokensUsed: 50
    };
  }
  
  if (promptLower.includes('comando') || promptLower.includes('executar')) {
    return {
      response: 'Comando processado com sucesso. Sistema em modo de desenvolvimento - todas as operaÃ§Ãµes estÃ£o sendo simuladas.',
      modelUsed: 'MockAI Command',
      tokensUsed: 30
    };
  }
  
  if (promptLower.includes('chat') || promptLower.includes('conversa') || promptLower.includes('como') || promptLower.includes('o que') || promptLower.includes('qual')) {
    const responses = [
      'Entendi sua pergunta! Como assistente da AUTVISION, posso ajudar vocÃª a automatizar tarefas, criar agentes inteligentes e otimizar seus processos. O que vocÃª gostaria de automatizar primeiro?',
      'Ã“tima pergunta! A AUTVISION oferece criaÃ§Ã£o de agentes personalizados, automaÃ§Ã£o de rotinas e integraÃ§Ã£o com diversas plataformas. Qual Ã¡rea do seu negÃ³cio vocÃª gostaria de otimizar?',
      'Interessante! Posso ajudar vocÃª a configurar automaÃ§Ãµes especÃ­ficas para suas necessidades. VocÃª tem algum processo repetitivo que gostaria de automatizar?',
      'Perfeito! Com a AUTVISION vocÃª pode criar agentes para atendimento, anÃ¡lise de dados, gestÃ£o de redes sociais e muito mais. Qual tipo de agente seria mais Ãºtil para vocÃª?',
      'Claro! Estou aqui para orientar vocÃª sobre as funcionalidades da plataforma. VocÃª gostaria de saber sobre criaÃ§Ã£o de agentes, automaÃ§Ã£o de tarefas ou integraÃ§Ã£o com outras ferramentas?',
      'Excelente! A AUTVISION permite automatizar desde tarefas simples atÃ© processos complexos. Conte-me mais sobre o que vocÃª precisa automatizar.',
      'Que bom que perguntou! Posso ajudar vocÃª a escolher os melhores agentes para suas necessidades especÃ­ficas. Qual Ã© o seu principal desafio atualmente?'
    ];
    
    return {
      response: responses[Math.floor(Math.random() * responses.length)],
      modelUsed: 'Vision AI',
      tokensUsed: 45
    };
  }
  
  // Resposta genÃ©rica mais Ãºtil
  const genericResponses = [
    'OlÃ¡! ğŸ‘‹ Sou o VISION, seu assistente inteligente na AUTVISION. Como posso ajudar vocÃª hoje? Posso orientar sobre criaÃ§Ã£o de agentes, automaÃ§Ã£o de processos ou integraÃ§Ã£o de ferramentas.',
    'Oi! Estou aqui para ajudar vocÃª a aproveitar ao mÃ¡ximo a plataforma AUTVISION. VocÃª gostaria de saber sobre nossos agentes inteligentes ou como automatizar suas tarefas?',
    'Seja bem-vindo! ğŸš€ Como assistente da AUTVISION, posso ajudar vocÃª a criar soluÃ§Ãµes de automaÃ§Ã£o personalizadas. Qual Ã© o seu objetivo principal?',
    'OlÃ¡! Que bom ter vocÃª aqui na AUTVISION. Posso ajudar vocÃª a configurar agentes, automatizar rotinas ou integrar sistemas. O que vocÃª precisa?'
  ];
  
  return {
    response: genericResponses[Math.floor(Math.random() * genericResponses.length)],
    modelUsed: 'Vision AI',
    tokensUsed: 35
  };
}

/**
 * ğŸ”§ Gera resposta mock baseada no schema JSON
 */
function generateMockJsonResponse(schema) {
  if (schema.type === 'object' && schema.properties) {
    const mockResponse = {};
    
    Object.keys(schema.properties).forEach(key => {
      const property = schema.properties[key];
      
      if (property.type === 'array' && property.items) {
        if (key === 'insights') {
          mockResponse[key] = [
            {
              type: 'performance',
              message: 'Sistema funcionando em modo de desenvolvimento. Backend serÃ¡ configurado em breve.',
              priority: 'medium'
            },
            {
              type: 'users',
              message: 'Chat simulado ativo. Todas as funcionalidades estÃ£o sendo testadas.',
              priority: 'low'
            },
            {
              type: 'system',
              message: 'Deploy para Vercel em preparaÃ§Ã£o. IntegraÃ§Ã£o LLM real serÃ¡ ativada.',
              priority: 'high'
            }
          ];
        } else {
          mockResponse[key] = ['Item mock 1', 'Item mock 2'];
        }
      } else if (property.type === 'string') {
        if (key === 'response') {
          mockResponse[key] = 'Sistema AUTVISION operacional em modo de desenvolvimento. Preparando para deploy...';
        } else if (key === 'action') {
          mockResponse[key] = 'mock_action';
        } else {
          mockResponse[key] = `Mock ${key}`;
        }
      } else if (property.type === 'boolean') {
        mockResponse[key] = key === 'execute' ? false : true;
      } else if (property.type === 'number') {
        mockResponse[key] = Math.floor(Math.random() * 100);
      }
    });
    
    return mockResponse;
  }
  
  // Fallback para schemas nÃ£o reconhecidos
  return {
    response: 'Mock response - sistema em desenvolvimento',
    success: true
  };
}

/**
 * ğŸ” Verifica se o backend estÃ¡ disponÃ­vel
 */
export const checkBackendHealth = async () => {
  try {
    const response = await axios.get('/health', {
      timeout: 5000,
      baseURL: API_BASE_URL
    });
    
    return {
      available: true,
      status: response.data?.status || 'ok',
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    console.warn('âš ï¸ Backend health check falhou:', error.message);
    return {
      available: false,
      error: error.message,
      timestamp: new Date().toISOString()
    };
  }
};

/**
 * ğŸ“Š Obter configuraÃ§Ãµes do sistema
 */
export const getSystemConfig = async () => {
  try {
    const response = await axios.get('/config/system', {
      timeout: 5000,
      baseURL: API_BASE_URL
    });
    
    return response.data;
  } catch (error) {
    console.warn('âš ï¸ NÃ£o foi possÃ­vel carregar configuraÃ§Ãµes do sistema:', error.message);
    
    // Retorna configuraÃ§Ãµes mock
    return {
      app_name: 'AutVision AI',
      app_version: '1.0.0',
      environment: ENVIRONMENT,
      llm_available: !LLM_MOCK_MODE,
      features: {
        voice_enabled: true,
        chat_enabled: true,
        admin_panel: true
      }
    };
  }
};

// ExportaÃ§Ãµes adicionais para compatibilidade
export default InvokeLLM;

/**
 * ğŸ¤– Gera respostas mock inteligentes baseadas em prompts comuns
 */
function generateIntelligentMockResponse() {
  const responses = [
    "ğŸ¤– OlÃ¡! Eu sou o Vision, seu assistente IA da AUTVISION. Como posso ajudar vocÃª hoje?",
    "âœ¨ O sistema AUTVISION estÃ¡ funcionando perfeitamente! Estou aqui para auxiliar com agentes, rotinas e automaÃ§Ãµes.",
    "ğŸš€ Pronto para criar algo incrÃ­vel? Posso ajudar vocÃª a configurar agentes inteligentes e rotinas automatizadas.",
    "ğŸ’¡ Que funcionalidade da AUTVISION vocÃª gostaria de explorar? Temos agentes especializados, rotinas personalizadas e muito mais!",
    "ğŸ¯ Backend em modo de desenvolvimento. Todas as funcionalidades principais estÃ£o operacionais para teste.",
    "ğŸ”§ Sistema em perfeito funcionamento! O que vocÃª gostaria de saber sobre a plataforma AUTVISION?"
  ];
  
  return responses[Math.floor(Math.random() * responses.length)];
}

// FunÃ§Ã£o real de upload de arquivo para o backend
export async function UploadFile({ file }) {
  const formData = new FormData();
  formData.append("file", file);
  // Ajuste o endpoint abaixo conforme o backend
  const response = await axios.post("/api/upload", formData, {
    headers: { "Content-Type": "multipart/form-data" },
  });
  return response.data;
}
