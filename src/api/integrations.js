// üî• INTEGRA√á√ÉO LLM REAL COM BACKEND AUTVISION
import axios from "./client";

// üåç Configura√ß√£o baseada em vari√°veis de ambiente
const API_BASE_URL = import.meta.env.VITE_API_BASE_URL || 'https://autvisionai-backend-five.vercel.app';
const LLM_MOCK_MODE = import.meta.env.VITE_LLM_MOCK_MODE === 'true';
const ENVIRONMENT = import.meta.env.VITE_ENVIRONMENT || 'development';

console.log('üîß LLM Config:', { 
  API_BASE_URL, 
  LLM_MOCK_MODE, 
  ENVIRONMENT 
});

/**
 * üß† Invoca a LLM atrav√©s do backend AutVision
 * @param {Object} params - Par√¢metros da chamada LLM
 * @param {string} params.prompt - Prompt para a LLM
 * @param {string} [params.systemPrompt] - System prompt opcional
 * @param {string} [params.agentId] - ID do agente (opcional)
 * @param {Object} [params.context] - Contexto adicional
 * @param {Object} [params.response_json_schema] - Schema JSON esperado (ser√° usado no system prompt)
 * @returns {Promise<Object>} Resposta da LLM
 */
export const InvokeLLM = async ({ 
  prompt, 
  systemPrompt, 
  context,
  response_json_schema 
}) => {
  // Se estiver em modo mock, retorna resposta mock diretamente
  if (LLM_MOCK_MODE) {
    console.log('ü§ñ Modo Mock ativado, retornando resposta simulada');
    return generateMockResponse(prompt, response_json_schema);
  }

  try {
    // Se h√° schema JSON, ajustar o system prompt para solicitar JSON
    let finalSystemPrompt = systemPrompt;
    if (response_json_schema && !finalSystemPrompt?.includes('JSON')) {
      finalSystemPrompt = `${systemPrompt || 'Voc√™ √© um assistente √∫til.'}\n\nIMPORTANTE: Responda APENAS com um JSON v√°lido seguindo exatamente este schema:\n${JSON.stringify(response_json_schema, null, 2)}`;
    }

    console.log('üß† Enviando prompt para LLM:', prompt.substring(0, 100) + '...');

    const response = await axios.post('/llm/invoke', {
      prompt,
      options: {
        systemPrompt: finalSystemPrompt,
        temperature: 0.7,
        maxTokens: 2048
      },
      context: {
        source: 'vision-commander',
        timestamp: new Date().toISOString(),
        environment: ENVIRONMENT,
        ...context
      }
    }, { 
      timeout: 15000, // Timeout de 15s para produ√ß√£o
      baseURL: API_BASE_URL
    });

    if (response.data?.success) {
      const llmResponse = response.data.data.content || response.data.data.response;
      
      // Se esperamos JSON, tentar fazer parse
      if (response_json_schema) {
        try {
          // Extrair JSON se a resposta cont√©m texto + JSON
          const jsonMatch = llmResponse.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            return JSON.parse(jsonMatch[0]);
          } else {
            return JSON.parse(llmResponse);
          }
        } catch (parseError) {
          console.warn('‚ùå Resposta n√£o √© JSON v√°lido, retornando como texto:', parseError);
          return { response: llmResponse, error: 'Invalid JSON response' };
        }
      }

      // Retorna resposta como string se n√£o √© JSON
      return { 
        response: llmResponse,
        modelUsed: response.data.data.modelUsed || 'IA',
        tokensUsed: response.data.data.tokensUsed || 0
      };
    } else {
      throw new Error(response.data?.error || 'Erro na resposta da LLM');
    }

  } catch (error) {
    console.error('‚ùå Erro ao chamar LLM:', error);
    
    // Fallback para desenvolvimento quando backend n√£o est√° dispon√≠vel
    if (error.response?.status === 404 || 
        error.code === 'ECONNREFUSED' || 
        error.code === 'ECONNABORTED' || 
        error.message?.includes('Network Error') ||
        error.message?.includes('timeout')) {
      
      console.warn('‚ö†Ô∏è Backend n√£o dispon√≠vel, usando resposta mock');
      return generateMockResponse(prompt, response_json_schema);
    }
    
    throw error;
  }
};

/**
 * üé≠ Gera resposta mock inteligente baseada no prompt e schema
 */
function generateMockResponse(prompt, response_json_schema) {
  // Se esperamos JSON estruturado
  if (response_json_schema) {
    return generateMockJsonResponse(response_json_schema);
  }

  // Respostas mock baseadas no contexto do prompt
  const promptLower = prompt.toLowerCase();
  
  if (promptLower.includes('analise') || promptLower.includes('an√°lise')) {
    return {
      response: 'Sistema AUTVISION funcionando perfeitamente em modo de desenvolvimento. Backend ser√° conectado em breve. Todas as funcionalidades core est√£o operacionais.',
      modelUsed: 'MockAI Dev',
      tokensUsed: 50
    };
  }
  
  if (promptLower.includes('comando') || promptLower.includes('executar')) {
    return {
      response: 'Comando processado com sucesso. Sistema em modo de desenvolvimento - todas as opera√ß√µes est√£o sendo simuladas.',
      modelUsed: 'MockAI Command',
      tokensUsed: 30
    };
  }
  
  if (promptLower.includes('chat') || promptLower.includes('conversa') || promptLower.includes('como') || promptLower.includes('o que') || promptLower.includes('qual')) {
    const responses = [
      'Entendi sua pergunta! Como assistente da AUTVISION, posso ajudar voc√™ a automatizar tarefas, criar agentes inteligentes e otimizar seus processos. O que voc√™ gostaria de automatizar primeiro?',
      '√ìtima pergunta! A AUTVISION oferece cria√ß√£o de agentes personalizados, automa√ß√£o de rotinas e integra√ß√£o com diversas plataformas. Qual √°rea do seu neg√≥cio voc√™ gostaria de otimizar?',
      'Interessante! Posso ajudar voc√™ a configurar automa√ß√µes espec√≠ficas para suas necessidades. Voc√™ tem algum processo repetitivo que gostaria de automatizar?',
      'Perfeito! Com a AUTVISION voc√™ pode criar agentes para atendimento, an√°lise de dados, gest√£o de redes sociais e muito mais. Qual tipo de agente seria mais √∫til para voc√™?',
      'Claro! Estou aqui para orientar voc√™ sobre as funcionalidades da plataforma. Voc√™ gostaria de saber sobre cria√ß√£o de agentes, automa√ß√£o de tarefas ou integra√ß√£o com outras ferramentas?',
      'Excelente! A AUTVISION permite automatizar desde tarefas simples at√© processos complexos. Conte-me mais sobre o que voc√™ precisa automatizar.',
      'Que bom que perguntou! Posso ajudar voc√™ a escolher os melhores agentes para suas necessidades espec√≠ficas. Qual √© o seu principal desafio atualmente?'
    ];
    
    return {
      response: responses[Math.floor(Math.random() * responses.length)],
      modelUsed: 'Vision AI',
      tokensUsed: 45
    };
  }
  
  // Resposta gen√©rica mais √∫til
  const genericResponses = [
    'Ol√°! üëã Sou o VISION, seu assistente inteligente na AUTVISION. Como posso ajudar voc√™ hoje? Posso orientar sobre cria√ß√£o de agentes, automa√ß√£o de processos ou integra√ß√£o de ferramentas.',
    'Oi! Estou aqui para ajudar voc√™ a aproveitar ao m√°ximo a plataforma AUTVISION. Voc√™ gostaria de saber sobre nossos agentes inteligentes ou como automatizar suas tarefas?',
    'Seja bem-vindo! üöÄ Como assistente da AUTVISION, posso ajudar voc√™ a criar solu√ß√µes de automa√ß√£o personalizadas. Qual √© o seu objetivo principal?',
    'Ol√°! Que bom ter voc√™ aqui na AUTVISION. Posso ajudar voc√™ a configurar agentes, automatizar rotinas ou integrar sistemas. O que voc√™ precisa?'
  ];
  
  return {
    response: genericResponses[Math.floor(Math.random() * genericResponses.length)],
    modelUsed: 'Vision AI',
    tokensUsed: 35
  };
}

/**
 * üîß Gera resposta mock baseada no schema JSON
 */
function generateMockJsonResponse(schema) {
  if (schema.type === 'object' && schema.properties) {
    const mockResponse = {};
    
    Object.keys(schema.properties).forEach(key => {
      const property = schema.properties[key];
      
      if (property.type === 'array' && property.items) {
        if (key === 'insights') {
          mockResponse[key] = [
            {
              type: 'performance',
              message: 'Sistema funcionando em modo de desenvolvimento. Backend ser√° configurado em breve.',
              priority: 'medium'
            },
            {
              type: 'users',
              message: 'Chat simulado ativo. Todas as funcionalidades est√£o sendo testadas.',
              priority: 'low'
            },
            {
              type: 'system',
              message: 'Deploy para Vercel em prepara√ß√£o. Integra√ß√£o LLM real ser√° ativada.',
              priority: 'high'
            }
          ];
        } else {
          mockResponse[key] = ['Item mock 1', 'Item mock 2'];
        }
      } else if (property.type === 'string') {
        if (key === 'response') {
          mockResponse[key] = 'Sistema AUTVISION operacional em modo de desenvolvimento. Preparando para deploy...';
        } else if (key === 'action') {
          mockResponse[key] = 'mock_action';
        } else {
          mockResponse[key] = `Mock ${key}`;
        }
      } else if (property.type === 'boolean') {
        mockResponse[key] = key === 'execute' ? false : true;
      } else if (property.type === 'number') {
        mockResponse[key] = Math.floor(Math.random() * 100);
      }
    });
    
    return mockResponse;
  }
  
  // Fallback para schemas n√£o reconhecidos
  return {
    response: 'Mock response - sistema em desenvolvimento',
    success: true
  };
}

/**
 * üîç Verifica se o backend est√° dispon√≠vel
 */
export const checkBackendHealth = async () => {
  try {
    const response = await axios.get('/health', {
      timeout: 5000,
      baseURL: API_BASE_URL
    });
    
    return {
      available: true,
      status: response.data?.status || 'ok',
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    console.warn('‚ö†Ô∏è Backend health check falhou:', error.message);
    return {
      available: false,
      error: error.message,
      timestamp: new Date().toISOString()
    };
  }
};

/**
 * üìä Obter configura√ß√µes do sistema
 */
export const getSystemConfig = async () => {
  try {
    const response = await axios.get('/config/system', {
      timeout: 5000,
      baseURL: API_BASE_URL
    });
    
    return response.data;
  } catch (error) {
    console.warn('‚ö†Ô∏è N√£o foi poss√≠vel carregar configura√ß√µes do sistema:', error.message);
    
    // Retorna configura√ß√µes mock
    return {
      app_name: 'AutVision AI',
      app_version: '1.0.0',
      environment: ENVIRONMENT,
      llm_available: !LLM_MOCK_MODE,
      features: {
        voice_enabled: true,
        chat_enabled: true,
        admin_panel: true
      }
    };
  }
};

// Exporta√ß√µes adicionais para compatibilidade
export default InvokeLLM;

/**
 * ü§ñ Gera respostas mock inteligentes baseadas em prompts comuns
 */
function generateIntelligentMockResponse() {
  const responses = [
    "ü§ñ Ol√°! Eu sou o Vision, seu assistente IA da AUTVISION. Como posso ajudar voc√™ hoje?",
    "‚ú® O sistema AUTVISION est√° funcionando perfeitamente! Estou aqui para auxiliar com agentes, rotinas e automa√ß√µes.",
    "üöÄ Pronto para criar algo incr√≠vel? Posso ajudar voc√™ a configurar agentes inteligentes e rotinas automatizadas.",
    "üí° Que funcionalidade da AUTVISION voc√™ gostaria de explorar? Temos agentes especializados, rotinas personalizadas e muito mais!",
    "üéØ Backend em modo de desenvolvimento. Todas as funcionalidades principais est√£o operacionais para teste.",
    "üîß Sistema em perfeito funcionamento! O que voc√™ gostaria de saber sobre a plataforma AUTVISION?"
  ];
  
  return responses[Math.floor(Math.random() * responses.length)];
}

// Fun√ß√£o real de upload de arquivo para o backend
export async function UploadFile({ file }) {
  const formData = new FormData();
  formData.append("file", file);
  // Ajuste o endpoint abaixo conforme o backend
  const response = await axios.post("/api/upload", formData, {
    headers: { "Content-Type": "multipart/form-data" },
  });
  return response.data;
}
